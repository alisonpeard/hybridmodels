{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2ffd69",
   "metadata": {},
   "source": [
    "# Spatial random forest for flood prediction\n",
    "Outcome variable: `floodfrac` represents presence/absence of flood in pixel (slightly misleading name.)\n",
    "\n",
    "\n",
    "Slightly dodgy, but modified this file to get rid of the annoying ImportError: Numba needs NumPy 1.21 or less. <br> _'~/miniconda3/envs/hybridmodels-modelbuild/lib/python3.10/site-packages/numba/__init__.py'_ <br>\n",
    "changed line 139 from ```elif numpy_version > (1, 21):``` to ```elif numpy_version > (1, 23):```\n",
    "Seems okay so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9be4ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "import model_utils\n",
    "\n",
    "RUNNAME = 'Current state'  # leave blank for run not being saved\n",
    "TO_REMOVE = []  # for trialing removing features\n",
    "NORM_TYPE = None\n",
    "MAX_DEPTH = None  # if I wanna change it for plotting trees\n",
    "TEST_EVENTS = model_utils.test_events\n",
    "# TEST_EVENTS = ['gombe_mossuril_0',\n",
    "#                'emnati_madagascar_3_0',\n",
    "#                'batsirai_menabe_0',\n",
    "#                'noul_vietnam_0',\n",
    "#                'roanu_satkania_0',\n",
    "#                'fani_eastindia_0_2',\n",
    "#                'fani_eastindia_0_3',\n",
    "#                'irma_jacksonvillesouth_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2186450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from os.path import join\n",
    "from os import mkdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# machine learning imports\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# plotting imports\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# helper function imports\n",
    "from importlib import reload\n",
    "import model_utils\n",
    "import data_utils\n",
    "import viz_utils\n",
    "from viz_utils import soge_colours, add_feature_label, format_event_string\n",
    "\n",
    "# fix for local issue with multiple proj installations\n",
    "import pyproj\n",
    "pyproj.datadir.get_data_dir()  # '/Users/alison/miniconda3/share/proj'\n",
    "pyproj.datadir.set_data_dir('/Users/alison/miniconda3/envs/hybridmodels-modelbuild/share/proj')\n",
    "pyproj.datadir.get_data_dir()\n",
    "\n",
    "# ignore annoying FutureWarning in RandomForest cell\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# set up the environment\n",
    "wd = join(\"..\", \"data\")\n",
    "imdir = join(wd, 'images')\n",
    "logdir = join(\"logfiles\")\n",
    "\n",
    "# folder for image results of this run\n",
    "runtime = datetime.now()\n",
    "try:\n",
    "    mkdir(join(imdir, 'rf_spatial', RUNNAME))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028f76c",
   "metadata": {},
   "source": [
    "## Load  data and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1685702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model_utils)\n",
    "gdf_orig = model_utils.load_spatial_data(wd)  # load gdfs with all features as one big gdf\n",
    "gdf = gdf_orig.copy(deep=True)  # keep copy in case gdf gets messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd2b3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique events: 59\n",
      "64 null values for precip in roanu_chittagong_0\n",
      "64 null values for elevation in roanu_chittagong_0\n",
      "64 null values for soiltemp2 in roanu_chittagong_0\n",
      "64 null values for soiltemp2_anom in roanu_chittagong_0\n",
      "64 null values for evi_anom in roanu_chittagong_0\n",
      "64 null values for evi in roanu_chittagong_0\n"
     ]
    }
   ],
   "source": [
    "# initial summary\n",
    "print(f\"Unique events: {gdf['event'].nunique()}\")\n",
    "# count number of nulls per feature and event\n",
    "for col in gdf.columns:\n",
    "    if gdf[col].isnull().sum() > 0:\n",
    "        num_nulls = gdf[col].isnull().sum()\n",
    "        events_with_null = [*gdf[gdf[col].isnull()]['event'].unique()]\n",
    "        print(f\"{num_nulls} null values for {col} in\", *events_with_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1de5c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option subset by a continent (can change to ocean etc. also)\n",
    "if False:\n",
    "    datasets = pd.read_csv(join(wd, 'csvs', 'current_datasets.csv'))[['event', 'region', 'continent']]\n",
    "    datasets = datasets[datasets['continent'] == 'africa']\n",
    "    events_list = [*datasets[['event', 'region']].agg('_'.join, axis=1)]\n",
    "    \n",
    "    TEST_EVENTS = [event for event in TEST_EVENTS if '_'.join(event.split('_')[:2]) in events_list]\n",
    "\n",
    "    gdf['event_region'] = gdf['event'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "    gdf = gdf[gdf['event_region'].isin(events_list)]\n",
    "    gdf = gdf.drop('event_region', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1b8d4",
   "metadata": {},
   "source": [
    "## Process data\n",
    "Process features for machine learning - order of functions matters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "821745be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of dataframe: 241,664 grid cells.\n",
      "\n",
      "After summarising LULC length: 241,664 grid cells.\n",
      "\n",
      "\n",
      "Before removing permanent water: 241,664\n",
      "After removing permanent water: 119,090\n",
      "\n",
      "\n",
      "Before exclusion mask: 119,090\n",
      "After exclusion mask: 91,617\n",
      "\n",
      "Columns with null rows ['precip', 'elevation', 'soiltemp2', 'soiltemp2_anom', 'evi_anom', 'evi']\n",
      "Number of null rows: 13\n",
      "Events with null rows: ['roanu_chittagong_0']\n",
      "\n",
      "Before dropping NaN rows: 91,617\n",
      "After dropping NaN rows: 91,604\n"
     ]
    }
   ],
   "source": [
    "reload(model_utils)\n",
    "reload(data_utils)\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "gdf = gdf_orig.copy(deep=True)  # if need to reset gdf\n",
    "print(f\"Initial length of dataframe: {len(gdf):,} grid cells.\\n\")\n",
    "\n",
    "# define columns and remove features being exluded\n",
    "notes = []\n",
    "columns = [*gdf.columns]\n",
    "gdf, columns, notes = model_utils.remove_feature(gdf, TO_REMOVE, columns, notes)\n",
    "                 \n",
    "# define predictive feature columns             \n",
    "features = columns.copy()\n",
    "features.remove('geometry')\n",
    "features.remove('floodfrac')\n",
    "features.remove('event')\n",
    "nfeatures = len(features)\n",
    "\n",
    "# sort binary and continuous features\n",
    "features_binary, features_continuous = data_utils.split_features_binary_continuous(data_utils.binary_keywords, features)\n",
    "\n",
    "# summarise LULC columns\n",
    "gdf, columns, features_binary, features_continuous, nfeatures, notes = model_utils.summarise_lulc(gdf, columns, features_binary, features_continuous, notes)\n",
    "features = features_binary + features_continuous\n",
    "print(f\"After summarising LULC length: {len(gdf):,} grid cells.\\n\")\n",
    "\n",
    "# binarise binary features\n",
    "for feature in features_binary:\n",
    "    gdf[feature] = gdf[feature].apply(lambda x: 1 if x > data_utils.floodthresh else 0).astype(int)\n",
    "\n",
    "# remove permanent water cells\n",
    "print(f\"\\nBefore removing permanent water: {len(gdf):,}\")\n",
    "gdf = gdf[gdf['dist_pw'] > 0]\n",
    "print(f\"After removing permanent water: {len(gdf):,}\\n\")\n",
    "notes.append('removed permanent water grid cells')\n",
    "\n",
    "# remove exclusion mask cells\n",
    "print(f\"\\nBefore exclusion mask: {len(gdf):,}\")\n",
    "gdf.loc[:, 'exclusion_mask'] = gdf['exclusion_mask'].apply(lambda x: 0 if np.isnan(x) else x)\n",
    "gdf = gdf[gdf['exclusion_mask'] != 1]\n",
    "notes.append('removed exclusion mask cells')\n",
    "print(f\"After exclusion mask: {len(gdf):,}\\n\")\n",
    "\n",
    "# remove rows containing NaN (for now)\n",
    "print(f\"Columns with null rows {gdf.columns[gdf.isnull().any()].tolist()}\")\n",
    "print(f\"Number of null rows: {sum(gdf.isnull().any(axis=1))}\")\n",
    "print(f\"Events with null rows: {[*gdf[gdf.isnull().any(axis=1)].event.unique()]}\\n\")\n",
    "print(f\"Before dropping NaN rows: {len(gdf):,}\")\n",
    "gdf = gdf.dropna()\n",
    "print(f\"After dropping NaN rows: {len(gdf):,}\")\n",
    "\n",
    "# normalise\n",
    "if NORM_TYPE is None:\n",
    "    gdf_normalised = gdf.copy()\n",
    "    for feature in features_continuous:\n",
    "        gdf_normalised = model_utils.normalise_feature(gdf_normalised, feature)\n",
    "    notes.append(f\"Normalised all {features_continuous}.\")\n",
    "else:\n",
    "    gdf_normalised = model_utils.normalise_per_subset(gdf, features_continuous, NORM_TYPE)\n",
    "    notes.append(f\"Normalised all {features_continuous} on a per-{NORM_TYPE} basis\")\n",
    "\n",
    "gdf_normalised['floodfrac'] = gdf_normalised['floodfrac'].astype(int)\n",
    "\n",
    "# finally, make sure only use final features for model\n",
    "features = model_utils.model_features\n",
    "columns = features + ['event', 'floodfrac', 'geometry']\n",
    "feature_binary, feature_continuous = data_utils.split_features_binary_continuous(data_utils.binary_keywords, features)\n",
    "gdf_normalised = gdf_normalised[columns]\n",
    "\n",
    "# remove test events from data for testing and plotting later\n",
    "gdf_test_unseen = gdf_normalised[gdf_normalised['event'].apply(lambda x: x in TEST_EVENTS)]\n",
    "gdf_normalised = gdf_normalised[~(gdf_normalised['event'].apply(lambda x: x in TEST_EVENTS))]\n",
    "gdf_test_unseen = gpd.overlay(gdf_test_unseen, gdf_normalised, how='difference')  # remove any overlapping bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "11c2bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before undersampling length: 79,229\n",
      "Number of ones: 17,346\n",
      "Number of zeros: 61,883\n",
      "After undersampling length: 34,692\n",
      "Number of ones: 17,346\n",
      "Number of zeros: 17,346\n",
      "\n",
      "Notes:\n",
      "-----\n",
      "\n",
      "['removed []', 'summarised lulc columns into three categories', 'removed permanent water grid cells', 'removed exclusion mask cells', \"Normalised all ['jrc_permwa', 'soiltemp2', 'wind_max', 'evi_spatial', 'elevation_spatial', 'evi_to_pw', 'dist_pw', 'pressure_avg', 'wind_avg', 'jrc_permwa_spatial', 'elevation_to_pw', 'soilcarbon', 'mangrove_spatial', 'slope_pw', 'mangrove_to_pw', 'evi_anom', 'mangrove', 'soiltemp2_anom', 'elevation', 'soilcarbon_to_pw', 'pressure_min', 'soilcarbon_spatial', 'precip', 'evi'].\", 'undersampled to same number of flood and non-flood pixels']\n"
     ]
    }
   ],
   "source": [
    "# random undersampling of training set\n",
    "print(f\"Before undersampling length: {len(gdf_normalised):,}\")\n",
    "print(f\"Number of ones: {gdf_normalised['floodfrac'].sum():,}\")\n",
    "print(f\"Number of zeros: {gdf_normalised['floodfrac'].count() - gdf_normalised['floodfrac'].sum():,}\")\n",
    "\n",
    "n = len(gdf_normalised)\n",
    "n1 = gdf_normalised['floodfrac'].sum()\n",
    "n0 = n - n1\n",
    "gdf_normalised, notes = model_utils.random_undersample(gdf_normalised, n0, n1, notes, columns, SEED)\n",
    "\n",
    "gdf_normalised = gdf_normalised.replace(np.nan, 0.)\n",
    "gdf_normalised['floodfrac'] = gdf_normalised['floodfrac'].astype(int)\n",
    "\n",
    "print(f\"After undersampling length: {len(gdf_normalised):,}\")\n",
    "print(f\"Number of ones: {gdf_normalised['floodfrac'].sum():,}\")\n",
    "print(f\"Number of zeros: {gdf_normalised['floodfrac'].count() - gdf_normalised['floodfrac'].sum():,}\")\n",
    "\n",
    "print(f\"\\nNotes:\\n-----\\n\\n{notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f05eb",
   "metadata": {},
   "source": [
    "# Random Forest: model\n",
    "## Cross-validate results with the tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f024ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf8b1a6480e44a293a4103154b80a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "run_rf = True\n",
    "\n",
    "if run_rf:\n",
    "    K = 5  # number of folds to use\n",
    "    best_params = np.load('best_params_rf.npy', allow_pickle=True).item()\n",
    "    if MAX_DEPTH is not None:\n",
    "        best_params['max_depth'] = MAX_DEPTH\n",
    "\n",
    "    X, y = gdf_normalised[features + ['event', 'geometry']], gdf_normalised['floodfrac']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "    \n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    \n",
    "    # separate from data for plotting later\n",
    "    X_train_toview = X_train.copy(deep=True)\n",
    "    X_test_seen = X_test.copy(deep=True)\n",
    "    \n",
    "    X_train = X_train[features]\n",
    "    X_test = X_test[features]\n",
    "\n",
    "    cv = KFold(n_splits=K, random_state=SEED, shuffle=True)\n",
    "\n",
    "    CSIs = []\n",
    "    TPs = []\n",
    "    FPs = []\n",
    "    TNs = []\n",
    "    FNs = []\n",
    "    rankings = []\n",
    "\n",
    "    for train_index, val_index in tqdm(cv.split(X_train)):\n",
    "        Xf_train, Xf_test = X_train.iloc[train_index, :], X_train.iloc[val_index, :]\n",
    "        yf_train, yf_test = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        rf = RandomForestClassifier(random_state=SEED, **best_params)\n",
    "        rf.fit(Xf_train, yf_train)\n",
    "        yf_pred = rf.predict(Xf_test)  # sample_weight=None\n",
    "\n",
    "        confusion_matrix = cm(yf_test, yf_pred)\n",
    "\n",
    "        if confusion_matrix.shape == (2, 2):\n",
    "            TP = confusion_matrix[1, 1]\n",
    "            FP = confusion_matrix[0, 1] #(row, col) = (true, pred)\n",
    "            FN = confusion_matrix[1, 0]\n",
    "            TN = confusion_matrix[0, 0]\n",
    "\n",
    "            TPs.append(TP)\n",
    "            FPs.append(FP)\n",
    "            FNs.append(FN)\n",
    "            TNs.append(TN)\n",
    "\n",
    "            CSI = TP / (TP + FP + FN)  # critical success index\n",
    "            CSIs.append(CSI)\n",
    "\n",
    "        rankings.append(pd.Series(rf.feature_importances_, index=features))\n",
    "        \n",
    "print(f\"mean of training CSIs: {np.mean(CSIs):.02%}\")\n",
    "print(f\"std. of training CSIs: {np.std(CSIs):.02%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fead518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "if MAX_DEPTH is None:\n",
    "    joblib.dump(rf, join(wd, 'results', 'unseen test set', 'rf.joblib'))\n",
    "    pd.DataFrame(features).to_csv(join(wd, 'results', 'unseen test set', 'rf_features.csv'), sep=',',index=False)\n",
    "else:\n",
    "    joblib.dump(rf, join(wd, 'results', 'unseen test set', f'rf_reduced_{MAX_DEPTH}.joblib'))\n",
    "    pd.DataFrame(features).to_csv(join(wd, 'results', 'unseen test set', f'rf_reduced_{MAX_DEPTH}_features.csv'), sep=',',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!say done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d9f76",
   "metadata": {},
   "source": [
    "## Evaluate on test set in seen location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6256f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from viz_utils import soge_colours\n",
    "\n",
    "\n",
    "def get_csi(y_test, y_pred):\n",
    "    \"\"\"Return CSI and relevant metrics for two arrays.\"\"\"\n",
    "    confusion_matrix = cm(y_test, y_pred)\n",
    "    if confusion_matrix.shape == (2, 2):\n",
    "        TP = confusion_matrix[1, 1]\n",
    "        FP = confusion_matrix[0, 1]  # (row, col) = (true, pred)\n",
    "        FN = confusion_matrix[1, 0]\n",
    "        TN = confusion_matrix[0, 0]\n",
    "        CSI = TP / (TP + FP + FN)    # CSI\n",
    "        \n",
    "        return CSI, TP, FP, FN, TN\n",
    "\n",
    "\n",
    "def confusion_label(y_test, y_pred):\n",
    "    \"\"\"Assign confusion label to a (true, predicted) pair.\"\"\"\n",
    "    if y_pred + y_test == 2:\n",
    "        return 'TP'\n",
    "    elif y_pred + y_test == 0:\n",
    "        return 'TN'\n",
    "    elif (y_test == 0) and (y_pred == 1):\n",
    "        return 'FP'\n",
    "    elif (y_test == 1) and (y_pred == 0):\n",
    "        return 'FN'\n",
    "    else:\n",
    "        print(f\"ISSUE: {y_test}, {y_pred}\")\n",
    "        return ''\n",
    "    \n",
    "\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add checker to check which events aren't empty\n",
    "\n",
    "# process data to view single event\n",
    "X_test_seen[f'flood_true'] = y_test\n",
    "X_test_seen[f'flood_pred'] = y_pred\n",
    "X_test_seen['confusion'] = X_test_seen[[f'flood_true', f'flood_pred']].apply(lambda row: confusion_label(row[f'flood_true'], row[f'flood_pred']), axis=1)\n",
    "events = X_test_seen['event'].unique()\n",
    "\n",
    "pwater = gdf_orig[gdf_orig['dist_pw'] <= 0]\n",
    "\n",
    "# loop through events\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i, event_to_view in enumerate(events[[1, 3, 4, 5]]):  \n",
    "    ax = axs[i]\n",
    "    \n",
    "    # subset event and calculate csi\n",
    "    X_test_event = X_test_seen[X_test_seen['event'] == event_to_view]\n",
    "    X_test_event = X_test_event.set_geometry(X_test_event.centroid)\n",
    "    csi_test_seen, _, _, _, _ = get_csi(X_test_event[f'flood_true'], X_test_event[f'flood_pred'])\n",
    "\n",
    "    # set up colors and markerstyles\n",
    "    bg_cmap = ListedColormap([soge_colours['beige'], soge_colours['light turquoise']])\n",
    "    markerstyles = {'TP': '+', 'FP': '+', 'TN': '_', 'FN': '_'}\n",
    "    markercolors = {'TP': 'darkgreen', 'FP': 'red', 'TN': 'darkgreen', 'FN': 'red'}\n",
    "    \n",
    "    # get subsets\n",
    "    X_test_pos = X_test_event[X_test_event.confusion.isin(['TP', 'FP'])]\n",
    "    X_test_neg = X_test_event[X_test_event.confusion.isin(['TN', 'FN'])]\n",
    "    bg = gdf_normalised[gdf_normalised['event'] == event_to_view]\n",
    "    pwater_toplot = pwater[pwater['event'] == event_to_view]\n",
    "\n",
    "    # plot background and test set points\n",
    "    bg.plot('floodfrac', categorical=True, cmap=bg_cmap, ax=ax, legend=True,\n",
    "            legend_kwds={'title': 'Flood (bool)', 'loc': 'upper left'}, zorder=0)\n",
    "    pwater_toplot.plot(color='darkblue', ax=ax, zorder=1)\n",
    "\n",
    "    X_test_pos.plot(color=X_test_pos['confusion'].map(markercolors), alpha=0.6,\n",
    "                    marker='+',  markersize=100, ax=ax, categorical=True, zorder=2)\n",
    "\n",
    "    X_test_neg.plot(color=X_test_neg['confusion'].map(markercolors), alpha=0.6,\n",
    "                      marker='_',  markersize=100, ax=ax, categorical=True, zorder=2)\n",
    "    \n",
    "    ax.set_title(f\"Results on test set {', '.join([x.capitalize() for x in event_to_view.split('_')])}\\nCSI: {csi_test_seen:.2%}\")\n",
    "    \n",
    "for ax in axs:\n",
    "    # clean up axses\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "fig.savefig(join(imdir, 'rf_spatial', RUNNAME, 'test_set_results.png'), dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ef5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!say done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535de04",
   "metadata": {},
   "source": [
    "## Feature rankings\n",
    "1. Impurity-based importance\n",
    "2. [Permutation-based importances](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impurity-based importances\n",
    "import viz_utils\n",
    "reload(viz_utils)\n",
    "runme = True\n",
    "savefig = True\n",
    "\n",
    "if runme:\n",
    "    stds = [np.std([rank[feature] for rank in rankings]) for feature in features]\n",
    "    means = [np.mean([rank[feature] for rank in rankings]) for feature in features]\n",
    "\n",
    "    mean_rankings = pd.Series(means, index=features)\n",
    "    mean_rankings = mean_rankings.rename(index=viz_utils.add_feature_label)\n",
    "    mean_rankings = mean_rankings.sort_values(ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    mean_rankings.plot.barh(yerr=stds, ax=ax, color='#86bf91')\n",
    "    ax.set_title(\"RF Impurity Feature Rankings\")\n",
    "    ax.set_ylabel(\"Mean impurity decrease\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(join(imdir, \"rf_spatial\", RUNNAME, f\"RF impurity feature rankings temporal\"), dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutatation importances\n",
    "run_pis = False\n",
    "\n",
    "# use critical success index to evaluate model according to scorer\n",
    "def csi(y_true, y_pred):\n",
    "    \"\"\"Critical success index\"\"\"\n",
    "    confusion_matrix = cm(y_true, y_pred)\n",
    "    TP = confusion_matrix[1, 1]\n",
    "    FP = confusion_matrix[0, 1]  # (row, col) = (true, pred)\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    csi = TP / (TP + FP + FN)  # critical success index\n",
    "    return csi\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    test_predictions = model.predict(test_features)\n",
    "    csi_val = csi(test_labels, test_predictions)\n",
    "    print('Model Performance')\n",
    "    print('CSI = {:0.2f}%.'.format(csi_val))  # should * 100 for next run\n",
    "    \n",
    "    \n",
    "    return csi_val\n",
    "\n",
    "csi_scorer = make_scorer(csi)\n",
    "\n",
    "if run_pis:\n",
    "    savefig = True\n",
    "\n",
    "    best_params = np.load('best_params_rf.npy', allow_pickle=True).item()\n",
    "    pi = permutation_importance(rf, X_test, y_test, scoring=csi_scorer, random_state=SEED)\n",
    "    mean_pis = pd.Series(pi.importances_mean, index=features)\n",
    "    mean_pis = mean_pis.rename(index=add_feature_label)\n",
    "    mean_pis = mean_pis.sort_values(ascending=True)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    mean_pis.plot.barh(yerr=pi.importances_std, ax=ax, color='#86bf91')\n",
    "    ax.set_title(\"Feature importances using permutation on full model\")\n",
    "    ax.set_ylabel(\"Mean CSI decrease\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(join(imdir, \"rf_spatial\", f\"RF permutation feature rankings temporal\"), dpi=500)\n",
    "        fig.savefig(join(imdir, \"rf_spatial\", RUNNAME, f\"RF permutation feature rankings temporal\"), dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbe72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! say done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5fc338",
   "metadata": {},
   "source": [
    "# Evaluate on an unseen subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38171e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_test_unseen_orig = gdf_test_unseen.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view predictions on the test event\n",
    "def get_csi(y_test_event, y_pred_event):\n",
    "    confusion_matrix = cm(y_test_event, y_pred_event)\n",
    "\n",
    "    TP_test = confusion_matrix[1, 1]\n",
    "    FP_test = confusion_matrix[0, 1]\n",
    "    FN_test = confusion_matrix[1, 0]\n",
    "    TN_test = confusion_matrix[0, 0]\n",
    "\n",
    "    CSI_test_unseen = TP_test / (TP_test + FP_test + FN_test)  # critical success index\n",
    "    return CSI_test_unseen\n",
    "\n",
    "\n",
    "X_test_event, y_test_event = gdf_test_unseen[features], gdf_test_unseen['floodfrac']\n",
    "y_pred_event = rf.predict(X_test_event)\n",
    "\n",
    "CSI_test_unseen = get_csi([*y_test_event], y_pred_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4317ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_event = gdf_test_unseen['floodfrac'].to_numpy()\n",
    "gdf_test_unseen['flood_pred'] = y_pred_event\n",
    "# gdf_test_unseen['floodfrac'] = gdf_test_unseen['floodfrac'].apply(lambda x: 1 if x=='Flood' else 0)\n",
    "gdf_test_unseen['error_type'] = gdf_test_unseen[['floodfrac', 'flood_pred']].apply(lambda row: confusion_label(row[0], row[1]), axis=1)\n",
    "\n",
    "# make flood categorical for plotting\n",
    "gdf_test_unseen['floodfrac'] = gdf_test_unseen['floodfrac'].apply(lambda x: 'No flood' if x==0 else 'Flood')\n",
    "gdf_test_unseen['flood_pred'] = gdf_test_unseen['flood_pred'].apply(lambda x: 'No flood' if x==0 else 'Flood')\n",
    "\n",
    "gdf_test_unseen.to_file(join(wd, 'results', 'unseen test set', 'unseen_results.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e590872",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSIs_test_unseen = []\n",
    "for event in TEST_EVENTS:\n",
    "    if event in [*gdf.event.unique()]:\n",
    "        gdf_subevent = gdf_test_unseen[gdf_test_unseen.event==event]\n",
    "        gdf_subevent['floodfrac'] = gdf_subevent['floodfrac'].apply(lambda x: 0 if x=='No flood' else 1)\n",
    "        X_test_subevent, y_test_subevent = gdf_subevent[features], gdf_subevent['floodfrac']\n",
    "        y_pred_subevent = rf.predict(X_test_subevent)\n",
    "        CSIs_test_unseen.append(get_csi(y_test_subevent, y_pred_subevent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in TEST_EVENTS:\n",
    "    print(event, \": \", event in [*gdf.event.unique()])\n",
    "    if event not in [*gdf.event.unique()]:\n",
    "        TEST_EVENTS.remove(event)  # remove any missing events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_event_string(string):\n",
    "    items = string.split('_')\n",
    "    if len(items) == 3:\n",
    "        storm, region, subregion = items\n",
    "        return f\"{storm.capitalize()} {region.capitalize()} (tile {subregion})\"\n",
    "    elif len(items) == 4:\n",
    "        storm, region0, region1, subregion = items\n",
    "        region = ' '.join([region0, f\"({region1})\"])\n",
    "        return f\"{storm.capitalize()} {region.capitalize()} (tile {subregion})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "\n",
    "# set colours\n",
    "pwater_col = '#034b80'\n",
    "mask_col = soge_colours['orange']\n",
    "cmap_flood = {'Flood': soge_colours['light turquoise'],\n",
    "              'No flood': soge_colours['beige']}\n",
    "\n",
    "cmap_contingency = {'TP': soge_colours['light turquoise'],\n",
    "             'FN': soge_colours['medium turquoise'],\n",
    "             'TN': soge_colours['beige'],\n",
    "             'FP': soge_colours['yellowish green'],\n",
    "             'mask': mask_col,\n",
    "             'perm. water': pwater_col\n",
    "            }\n",
    "cm_categories = ListedColormap([soge_colours['medium turquoise'],\n",
    "                                soge_colours['yellowish green'],\n",
    "                                soge_colours['beige'],\n",
    "                                soge_colours['light turquoise']])\n",
    "\n",
    "nevents = len(TEST_EVENTS)\n",
    "\n",
    "nrows = (nevents - 1) // 4 + 1\n",
    "ncols = 4 if nevents >= 4 else nevents\n",
    "fig = plt.figure(figsize=(15, 3 * nrows))\n",
    "gs = fig.add_gridspec(nrows, ncols)\n",
    "\n",
    "for i, test_event in enumerate(TEST_EVENTS):\n",
    "    ax = fig.add_subplot(gs[i // 4, i % 4])\n",
    "    \n",
    "    to_plot = gdf_test_unseen[gdf_test_unseen.event == test_event]\n",
    "\n",
    "    # plot background\n",
    "    gdf_orig[gdf_orig.event == test_event].plot(color=mask_col, ax=ax, zorder=0, label='mask')\n",
    "    pwater[pwater.event == test_event].plot(color=pwater_col, ax=ax, zorder=1, label='permanent water')\n",
    "    \n",
    "    # plot contingency\n",
    "    to_plot.plot(color=to_plot['error_type'].map(cmap_contingency), categorical=True,  # cmap=cm_categories,\n",
    "                 legend=False, legend_kwds={'loc': 'upper left'}, ax=ax,\n",
    "                 zorder=2)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xlabel(f'{format_event_string(test_event)}\\n{CSIs_test_unseen[i]:.2%}')\n",
    "    \n",
    "# add manual legend\n",
    "custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=10, color=color) for color in cmap_contingency.values()]\n",
    "leg_points = ax.legend(custom_points, cmap_contingency.keys())\n",
    "ax.add_artist(leg_points)\n",
    "\n",
    "# clean up\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.set_xlabel(f'{format_event_string(test_event)}\\n{CSIs_test_unseen[i]:.2%}')\n",
    "\n",
    "fig.suptitle(f\"Contingency Plots\\nMean CSI: {CSI_test_unseen:.2%}\",\n",
    "                 fontsize=18);\n",
    "\n",
    "\n",
    "# format output png and save\n",
    "outpath = join(imdir, 'rf_spatial', RUNNAME, f'unseen_regions_contingency.png')\n",
    "fig.savefig(outpath, bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668548c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! say finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf9624",
   "metadata": {},
   "source": [
    "## Log the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGTHISRUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ebe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOGTHISRUN:\n",
    "    # add to the metrics csv\n",
    "    metrics_dict = {\n",
    "        'date': runtime.strftime(\"%Y-%m-%d %Hh%Mm\"),\n",
    "        'runname': RUNNAME,\n",
    "        'notes': [notes],\n",
    "        'nevents': gdf_normalised['event'].nunique(),\n",
    "        'nfeatures': nfeatures,\n",
    "        'features': [[*features]],\n",
    "        'test events': [[*TEST_EVENTS]],\n",
    "        'nzeros': n0,\n",
    "        'nones': n1,\n",
    "        'folds': K,\n",
    "        'SEED': SEED,\n",
    "        'best_params': [best_params],\n",
    "        'mean train CSI': np.mean(CSIs),\n",
    "        'std train CSIs': np.std(CSIs),\n",
    "        'test CSI (unseen)': CSI_test_unseen,\n",
    "        'impurity-based rankings': [[*mean_rankings.index]],\n",
    "        'impurity-based scores': [mean_rankings.values],\n",
    "        'perm-based rankings': [[*mean_pis.index]] if run_pis else \"not done\",\n",
    "        'perm-based scores': [mean_pis.values] if run_pis else \"not done\",\n",
    "        'results folder': outpath\n",
    "    }\n",
    "\n",
    "    all_metrics = pd.read_csv(join(wd, 'images', 'rf_spatial', 'metrics.csv')).astype(str)\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='columns', dtype='object').astype(str)\n",
    "    new_metrics = all_metrics.append(metrics_df)\n",
    "    new_metrics = new_metrics.drop_duplicates()\n",
    "    new_metrics.to_csv(join(wd, 'images', 'rf_spatial', 'metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab775374",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics[['runname', 'mean train CSI', 'std train CSIs', 'test CSI (seen)',\n",
    "       'test CSI (unseen)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "! say done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36393dff",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196bc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_kwargs = {'bbox_inches': 'tight', 'dpi': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_csis = new_metrics['test CSI (unseen)'].astype(float).values\n",
    "seen_csis = new_metrics['test CSI (seen)'].astype(float).values\n",
    "index = [*range(len(unseen_csis))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(index, unseen_csis, label='unseen CSI')\n",
    "ax.plot(index, seen_csis, label='seen CSI')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Run number')\n",
    "ax.set_ylabel('Unseen CSI')\n",
    "ax.set_title('Progress');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda75bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(join(wd, 'results', 'unseen test set', 'unseen_results.gpkg'))\n",
    "\n",
    "TP = gdf[gdf['error_type'] == 'TP']\n",
    "TN = gdf[gdf['error_type'] == 'TN']\n",
    "FP = gdf[gdf['error_type'] == 'FP']\n",
    "FN = gdf[gdf['error_type'] == 'FN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8845749",
   "metadata": {},
   "source": [
    "## Feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435729d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of different features\n",
    "FEATURE = 'wind_avg'\n",
    "\n",
    "hist_kwargs = {'alpha': 0.8, 'density': True, 'bins': 20, 'linewidth': 0.3, 'edgecolor': 'grey'}\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.hist(TN[FEATURE], color=soge_colours['beige'], label='TN', **hist_kwargs)\n",
    "ax.hist(FP[FEATURE], color=soge_colours['yellowish green'], label='FP', **hist_kwargs)\n",
    "ax.hist(FN[FEATURE], color=soge_colours['medium turquoise'], label='FN', **hist_kwargs)\n",
    "ax.hist(TP[FEATURE], color=soge_colours['light turquoise'], label='TP', **hist_kwargs)\n",
    "ax.legend()\n",
    "ax.set_title(f\"{viz_utils.labels[FEATURE].capitalize()} distribution for four error types\")\n",
    "ax.set_xlabel('Data')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.hist(gdf[FEATURE], color=soge_colours['orange'], label='All', **hist_kwargs)\n",
    "ax.set_title(f\"{viz_utils.labels[FEATURE].capitalize()} distribution overall\")\n",
    "ax.set_xlabel('Data')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(join(imdir, \"rf_spatial\", RUNNAME, f'distribution_{FEATURE}.png'), **image_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988750f2",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2adbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots of features\n",
    "FEATURE = 'elevation'\n",
    "\n",
    "feat_arrs = []\n",
    "feat_labs = []\n",
    "colours = {'full unseen dataset': viz_utils.soge_colours['orange'],\n",
    "          'TN': viz_utils.soge_colours['beige'],\n",
    "          'FN': viz_utils.soge_colours['medium turquoise'],\n",
    "          'FP': viz_utils.soge_colours['yellowish green'],\n",
    "          'TP': viz_utils.soge_colours['light turquoise']\n",
    "         }\n",
    "\n",
    "# first entry is for entire dataset\n",
    "feat_arrs.append([*gdf[FEATURE]])\n",
    "feat_labs.append('full unseen dataset')\n",
    "\n",
    "for error_type in sorted(gdf['error_type'].unique()):\n",
    "    gdf_error_type = gdf[gdf['error_type'] == error_type]\n",
    "    feat_arrs.append([*gdf_error_type[FEATURE]])\n",
    "    feat_labs.append(error_type)\n",
    "    \n",
    "colours = [colours[x] for x in feat_labs]\n",
    "    \n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "bp = ax.boxplot(feat_arrs, vert=False, patch_artist=True);\n",
    "ax.set_title(viz_utils.labels[FEATURE].capitalize())\n",
    "ax.set_yticklabels(feat_labs, rotation=0, fontsize=12, va='center')\n",
    "\n",
    "for patch, colour in zip(bp['boxes'], colours):\n",
    "    patch.set_facecolor(colour)\n",
    "    patch.set_alpha(0.8)\n",
    "    patch.set_edgecolor('darkgrey')\n",
    "\n",
    "# changing style of fliers\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker ='.',\n",
    "              color ='blue',\n",
    "              alpha = 0.5,\n",
    "              markersize=0.1)\n",
    "    \n",
    "# changing color and linewidth of\n",
    "# whiskers\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color ='black',\n",
    "                linewidth = 1.5,\n",
    "                linestyle =\":\")\n",
    "\n",
    "# changing color and linewidth of\n",
    "# caps\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color ='black',\n",
    "            linewidth = 2)\n",
    "    \n",
    "# changing color and linewidth of\n",
    "# medians\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='black',\n",
    "               linewidth = 3)\n",
    "    \n",
    "fig.savefig(join(imdir, \"rf_spatial\", RUNNAME, f'error_distributions_{FEATURE}.png'), **image_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2e5c6",
   "metadata": {},
   "source": [
    "## Flood types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a182cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = '8'\n",
    "\n",
    "def parse_regions(row):\n",
    "    if row['sub1'] is None:\n",
    "        return row['region']\n",
    "    else:\n",
    "        return '_'.join([row['region'], row['sub0']])\n",
    "    \n",
    "    \n",
    "def parse_subregions(row):\n",
    "    if row['sub1'] is None:\n",
    "        return row['sub0']\n",
    "    else:\n",
    "        return row['sub1']\n",
    "    \n",
    "gdf = gpd.read_file(join(wd, 'results', 'unseen test set', 'unseen_results.gpkg'))\n",
    "gdf[['storm', 'region', 'sub0', 'sub1']] = gdf['event'].str.split('_', expand=True)\n",
    "\n",
    "metadata = pd.read_csv(join(wd, 'csvs', 'current_datasets.csv'), index_col=['event', 'region'])\n",
    "\n",
    "gdf['region'] = gdf.apply(lambda row: parse_regions(row), axis=1)\n",
    "gdf['subregion'] = gdf.apply(lambda row: parse_regions(row), axis=1)\n",
    "gdf['continent'] = gdf.apply(lambda row: metadata.loc[(row['storm'], row['region'])]['continent'], axis=1)\n",
    "\n",
    "events = []\n",
    "for event in gdf['event'].unique():\n",
    "    gdf_event = gdf[gdf['event'] == event]\n",
    "    continent = gdf_event.continent.mode()[0]\n",
    "    hydrobasins = gpd.read_file(join(wd, 'hydrobasin', f'hybas_{continent}_lev01-12_v1c', f'hybas_{continent}_lev{LEVEL.zfill(2)}_v1c.shp'),\n",
    "                               mask=gdf_event)\n",
    "    gdf_event_type = gpd.overlay(gdf_event, hydrobasins[['COAST', 'geometry']], how='intersection')\n",
    "    events.append(gdf_event_type)\n",
    "\n",
    "gdf = pd.concat(events)\n",
    "TP = gdf[gdf['error_type'] == 'TP']\n",
    "TN = gdf[gdf['error_type'] == 'TN']\n",
    "FP = gdf[gdf['error_type'] == 'FP']\n",
    "FN = gdf[gdf['error_type'] == 'FN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = 'COAST'\n",
    "\n",
    "hist_kwargs = {'alpha': 0.8, 'density': False, 'bins': 2, 'linewidth': 0.3, 'edgecolor': 'grey'}\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = axs[0]\n",
    "\n",
    "ax.hist([TN[FEATURE], FP[FEATURE], FP[FEATURE], TP[FEATURE]],\n",
    "        color=[soge_colours['beige'], soge_colours['yellowish green'], soge_colours['medium turquoise'], soge_colours['light turquoise']],\n",
    "        label=['TN', 'TP', 'FP', 'TP'],\n",
    "        **hist_kwargs)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(f\"{viz_utils.labels[FEATURE].capitalize()} counts for four error types\")\n",
    "ax.set_xlabel('Data')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks([0.25, 0.75])\n",
    "ax.set_xticklabels(['inland', 'coastal'])\n",
    "\n",
    "ax = axs[1]\n",
    "ax.hist(gdf[FEATURE], color=soge_colours['orange'], label='All', **hist_kwargs)\n",
    "ax.set_title(f\"{viz_utils.labels[FEATURE].capitalize()} counts overall\")\n",
    "ax.set_xticks([0.25, 0.75])\n",
    "ax.set_xticklabels(['inland', 'coastal'])\n",
    "ax.set_xlabel('Data')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(join(imdir, \"rf_spatial\", RUNNAME, f'floodtype_{FEATURE}.png'), **image_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a98e40",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (Optional)\n",
    "## 1. Random CV\n",
    "From [this tutorial](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6131a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# set up random grid search\n",
    "randomcv = False\n",
    "\n",
    "# look at base case\n",
    "SEED = 1\n",
    "rf = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "# train-test split\n",
    "X, y = gdf_normalised[features], gdf_normalised['flood']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "\n",
    "if randomcv:\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    pprint(random_grid)\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100,\n",
    "                                   cv=3, scoring=csi_scorer, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    print(rf_random.best_params_)\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    np.save('best_params_rf.npy', grid_search.best_params_) \n",
    "    \n",
    "    \n",
    "    base_model = RandomForestClassifier(random_state=SEED)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    base_csi = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "    best_random = grid_search.best_estimator_\n",
    "    random_csi = evaluate(best_random, X_test, y_test)\n",
    "\n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (random_csi - base_csi) / base_csi))\n",
    "\n",
    "    !say done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7455b4c",
   "metadata": {},
   "source": [
    "## 2. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = False\n",
    "\n",
    "if gridsearch:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [40, 45, 50, 55, 60],\n",
    "        'max_features': ['auto'],\n",
    "        'min_samples_leaf': [1, 2, 3],\n",
    "        'min_samples_split': [1, 2, 3],\n",
    "        'n_estimators': [500, 1000, 1500]\n",
    "    }\n",
    "\n",
    "    # Create a based model\n",
    "    rf = RandomForestClassifier(random_state=SEED)\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=csi_scorer, \n",
    "                              cv=3, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    np.save('best_params_rf.npy', grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62e962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca0202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybridmodels-modelbuild",
   "language": "python",
   "name": "hybridmodels-modelbuild"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
